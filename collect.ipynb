{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "author_papers = {}\n",
    "\n",
    "# get all the papers from an author\n",
    "with open(\"data.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "    for paper in data:\n",
    "        if paper['s2data'] == None: continue\n",
    "        authors = paper['s2data']['authors']\n",
    "        for author in authors: \n",
    "            if author['authorId'] in author_papers: continue\n",
    "            author_id = author['authorId']\n",
    "            response = requests.get(\"https://api.semanticscholar.org/graph/v1/author/{}/papers\".format(author_id) +\n",
    "                        \"?fields=url,title,year,authors,abstract,publicationDate\")\n",
    "            while 'message' in response.json() and response.json()['message'] == 'Too Many Requests':\n",
    "                print(\"--- Waiting\")\n",
    "                time.sleep(300)\n",
    "            author_papers[author_id] = response.json()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the cosine similarity value of the two given texts\n",
    "def compute_cosine_similarity(text1, text2):\n",
    "    \n",
    "    # stores text in a list\n",
    "    list_text = [text1, text2]\n",
    "    \n",
    "    # converts text into vectors with the TF-IDF \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    vectorizer.fit_transform(list_text)\n",
    "    tfidf_text1, tfidf_text2 = vectorizer.transform([list_text[0]]), vectorizer.transform([list_text[1]])\n",
    "    \n",
    "    # computes the cosine similarity\n",
    "    cs_score = cosine_similarity(tfidf_text1, tfidf_text2)\n",
    "    \n",
    "    return np.round(cs_score[0][0],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paper info in hw2\n",
    "paper_dic = {}\n",
    "for key, value in author_papers.items():\n",
    "    for paper in value:\n",
    "        paper_dic[paper['paperId']] = paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each paper, get a list of relevant papers for each author\n",
    "dic = {}\n",
    "with open(\"data.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "    for paper in data:\n",
    "        if paper['s2data'] == None: continue\n",
    "        abstract = paper['s2data']['abstract']\n",
    "        authors = paper['s2data']['authors']\n",
    "        if paper['s2data']['paperId'] not in dic:\n",
    "            dic[paper['s2data']['paperId']] = {}\n",
    "                \n",
    "        for author in authors: \n",
    "            # check relevancy of the author's papers\n",
    "            author_id = author['authorId']\n",
    "            rel_paper_lst = []\n",
    "            for rel_paper in author_papers[author_id]:\n",
    "                rel_abstract = rel_paper['abstract']\n",
    "                if abstract == None or rel_abstract == None:\n",
    "                    score = 0\n",
    "                else:\n",
    "                    score = compute_cosine_similarity(abstract, rel_abstract)\n",
    "                    \n",
    "                rel_paper_lst.append({\n",
    "                    \"source\": paper['s2data']['paperId'],\n",
    "                    \"target\": rel_paper['paperId'],\n",
    "                    \"similarity\": score\n",
    "                })\n",
    "            paper_lst = sorted(rel_paper_lst, key=lambda d: d['similarity'], reverse=True)\n",
    "            \n",
    "            ret = []\n",
    "            for p in paper_lst:\n",
    "                temp = paper_dic[p['target']]\n",
    "                temp['score'] = p['similarity']\n",
    "                ret.append(temp)\n",
    "            dic[paper['s2data']['paperId']][author['authorId']] = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank based on the similarity score\n",
    "for k, v in dic.items():\n",
    "    for author, lst in v.items():\n",
    "        v[author] = [item for item in lst if item['paperId'] != k]\n",
    "        v[author].sort(key=lambda x: x['score'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append meta info \n",
    "import json\n",
    "with open(\"relevant3.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open(\"data.json\", \"r\") as file:\n",
    "    original = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the original paper information\n",
    "paper_mapper, author_mapper = {}, {}\n",
    "for p in original: \n",
    "    info = p['s2data']\n",
    "    if info is None: continue\n",
    "    \n",
    "    id = info['paperId']\n",
    "    paper_mapper[id] = {\n",
    "        \"url\": info['url'],\n",
    "        \"title\": info['title'],\n",
    "        \"abstract\": info[\"abstract\"],\n",
    "        \"year\": info[\"year\"],\n",
    "        \"citationCount\": info[\"citationCount\"],\n",
    "        \"referenceCount\": info[\"referenceCount\"],\n",
    "        \"tldr\": info[\"tldr\"],\n",
    "        \"authors\": info[\"authors\"],\n",
    "        \"externalIds\": info[\"externalIds\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "author_mapper = {}\n",
    "for p, author_dic in data.items():\n",
    "    for author_id, lst in author_dic.items():\n",
    "        if author_id in author_mapper: continue\n",
    "#         print(author_id)\n",
    "        # api call for author info\n",
    "        response = requests.get(\"https://api.semanticscholar.org/graph/v1/author/{}\".format(author_id) +\n",
    "                        \"?fields=name,aliases,affiliations,homepage,paperCount,citationCount,hIndex\")\n",
    "       \n",
    "        while 'message' in response.json() and response.json()['message'] == 'Too Many Requests':\n",
    "            print(\"Waiting ---\")\n",
    "            time.sleep(300)\n",
    "            response = requests.get(\"https://api.semanticscholar.org/graph/v1/author/{}\".format(author_id) +\n",
    "                        \"?fields=name,aliases,affiliations,homepage,paperCount,citationCount,hIndex\")\n",
    "            \n",
    "        author_mapper[author_id] = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"authors.json\", \"w\") as file:\n",
    "    file.write(json.dumps(author_mapper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all tldr. This is too slow.\n",
    "output = {}\n",
    "count = 0\n",
    "unique_papers = set()\n",
    "TLDRS = {}\n",
    "import time\n",
    "# add tldr for all relevant papers\n",
    "for p, author_dic in data.items():\n",
    "    for author_id, lst in author_dic.items():\n",
    "#         output[p][author_id] = []\n",
    "        for idx, rel_p in enumerate(lst):\n",
    "            rel_id = rel_p['paperId']\n",
    "            \n",
    "            response = requests.get(\"https://api.semanticscholar.org/graph/v1/paper/{}\".format(rel_id) +\n",
    "                        \"?fields=tldr\")\n",
    "       \n",
    "            while 'message' in response.json() and response.json()['message'] == 'Too Many Requests':\n",
    "                print(\"Waiting ----\")\n",
    "                time.sleep(300)\n",
    "                response = requests.get(\"https://api.semanticscholar.org/graph/v1/paper/{}\".format(rel_id) +\n",
    "            \"?fields=tldr\")\n",
    "            \n",
    "            if 'tldr' in response.json():\n",
    "                TLDRS[rel_id] = response.json()['tldr']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
